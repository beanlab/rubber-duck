import asyncio
import re
from pathlib import Path
import json
import yaml

import markdowndata

from functools import reduce
import operator

from ..gen_ai.gen_ai import Agent, AIClient
from ..utils.config_types import DuckContext, AssignmentFeedbackSettings, Gradable, RubricItemResponse
from ..utils.logger import duck_logger
from ..utils.message_utils import wait_for_message
from ..utils.protocols import ConversationComplete

ASSIGNMENT_NAME = str
SECTION = str
SECTION_NAME = str
RUBRIC_ITEM = str
REPORT_SECTION = str
FEEDBACK = str
SATISFACTORY = bool


class AssignmentFeedbackWorkflow:
    def __init__(self,
                 name: str,
                 send_message,
                 settings: AssignmentFeedbackSettings,
                 single_rubric_item_grader: Agent,
                 project_scanner_agent: Agent,
                 ai_client: AIClient,
                 read_url
                 ):
        self.name = name
        self._send_message = send_message
        self._settings = settings
        self._single_rubric_item_grader = single_rubric_item_grader
        self._project_scanner_agent = project_scanner_agent
        self._ai_client = ai_client
        self.read_url = read_url

        self._assignments_rubrics: dict[ASSIGNMENT_NAME: dict[SECTION: any]] = {}
        self._populate_assignments_rubrics()

    async def __call__(self, context: DuckContext):
        """
        Assumptions about rubric and report:
            - The structure rubric and report should match exactly. The nesting and the names should align.
            - safe_yaml.loads() from the rubric and mdd.loads() from the report should result in the same dictionary structure
            - The report content in the corresponding section in the rubric will be what will be graded for that rubric item
            - Headers and rubric items cannot be mixed on the same level

        Any top level headers in rubric yaml starting with '_' will be ignored.
        The grading will grade based on the rubric. If there are additional sections in the md, they will be ignored.

        Ideally, the project name provided in the config and the first level 1 header in the document align.
        If not, an agent scrubs the report to determine the corresponding report.

        Rubric and Project Report Example

        ```yaml
        Project Fruit:
           Section 1:
              Subsection 1:
                - Rubric item to talk about apples
              Section 2:
                - Rubric item to talk about bananas
           Section 2:
              - Rubric item to talk about cars
           _Section 3:
              - This rubric item and section will **not** be ignored because it is not a top level header

        _private_header:
            - This header, and anything in this section, will be ignored.
            - This may be useful for yaml anchors

        ```

        ```md
        # Project Fruit
        ## Section 1
        ### Subsection 1
        This is the section that would be graded on talking about apples.
        ### Subsection 2
        This section would be graded on talking about bananas
        ## Section 2
        This section would be graded on talking about cars
        ### Section 3
        This section would be ignored because it is not included in the rubric.
        it would **not** be graded or looked at.
        ```

        """
        try:
            # Send an initial message
            await self._send_message(context.thread_id,
                                     self._settings.get("initial_message",
                                                        "Content generated by AI may not be correct."))

            # Tell the user the supported assignments
            supported_assignments = ', '.join(f'**{assignment}**' for assignment in self._assignments_rubrics.keys())
            await self._send_message(context.thread_id,
                                     f"The supported assignments for grading are {supported_assignments}")

            # Get the report contents from the user
            report_contents = await self._get_report_contents(context)

            # Get the project name associated with the report
            valid_project_names = [assignment["name"] for assignment in self._settings['gradable_assignments']]
            project_name = await self._get_project_name_from_report(context, report_contents, valid_project_names)

            if project_message := self._get_project_specific_message(project_name):
                await self._send_message(context.thread_id, project_message)

            # break up the report into small pieces to grade with the associated rubric item
            tasks = [
                asyncio.create_task(self._single_item_grader(context, piece_name, report_section, rubric_item))
                for piece_name, rubric_item, report_section in
                self._flatten_report_and_rubric_items(report_contents, self._assignments_rubrics[project_name])
            ]

            # wait for the responses of all
            graded_items: list[tuple[list[SECTION_NAME], RubricItemResponse]] = await asyncio.gather(*tasks)

            # format and send the graded rubric items
            formatted_graded_items = self._format_graded_items(graded_items)

            await self._send_message(context.thread_id, formatted_graded_items)

        except ConversationComplete as e:
            print("\n\nEXCEPTION:\n\n", e)
            await self._send_message(context.thread_id, str(e))
            return

    def _get_project_specific_message(self, project_name):
        gradables = self._settings['gradable_assignments']
        for gradable in gradables:
            if gradable['name'] == project_name:
                return gradable.get('message')

    def _get_expected_md_format(self, rubric):
        as_md = self._dict_to_md(rubric)
        return (f""
                f"```md\n"
                f"{as_md}"
                f"```")

    def _flatten_report_and_rubric_items(self, report_contents, rubric) -> list[
        tuple[list[SECTION_NAME], RUBRIC_ITEM, REPORT_SECTION]]:
        def helper_func(name, rubric_section, report_section):
            for section_name in rubric_section.keys():
                name.append(section_name)
                if isinstance(rubric_section[section_name], dict):
                    yield from helper_func(name, rubric_section[section_name], report_section[section_name])
                elif isinstance(rubric_section[section_name], list):
                    for section_item in rubric_section[section_name]:
                        yield name[::], section_item, report_section[section_name]
                name.pop(-1)

        try:
            report = markdowndata.loads(report_contents)
            flattened = list(helper_func([], rubric, report))
            return flattened
        except KeyError as e:
            raise ConversationComplete(f"Unable to find header {e} in the report. \n"
                                       f"The expected format is as follows: {self._get_expected_md_format(rubric)}")

    def _format_single_response(self, response: RubricItemResponse):
        emoji = ':white_check_mark:' if response['satisfactory'] else ':x:'
        justification = response['justification']
        return f'{emoji} **{response["rubric_item"]}** - {justification}'

    def _format_graded_items(self, results: list[tuple[list[SECTION_NAME], RubricItemResponse]]):

        results = [
            (name, self._format_single_response(rubric_item_response))
            for (name, rubric_item_response) in results
        ]

        result = self._unflatten_dictionary(results)
        return self._dict_to_md(result)

    def _get_nested(self, d, keys):
        return reduce(operator.getitem, keys, d)

    def _set_nested(self, d, keys, value):
        *prefix, last = keys
        parent = reduce(lambda acc, k: acc.setdefault(k, {}), prefix, d)
        parent.setdefault(last, []).append(value)

    def _unflatten_dictionary(self, results):
        unflattened = {}
        for keys, formatted in results:
            self._set_nested(unflattened, keys, formatted)
        return unflattened

    def _get_rubric_content(self, assignment: Gradable):
        if 'rubric_path' in assignment:
            instructions = Path(assignment['rubric_path']).read_text(encoding="utf-8")
        else:
            raise ValueError(f"You must provide an 'rubric_path' for {assignment['name']}")
        return instructions

    def _remove_private_keys_from_rubric(self, rubric):
        keys_to_remove = [key for key, val in rubric.items() if key[0] == '_']
        for key in keys_to_remove:
            rubric.pop(key)
        return rubric

    def _populate_assignments_rubrics(self):
        try:
            for assignment in self._settings["gradable_assignments"]:
                raw_rubric_content = self._get_rubric_content(assignment)
                rubric_content = yaml.safe_load(raw_rubric_content)
                rubric_content = self._remove_private_keys_from_rubric(rubric_content)
                self._assignments_rubrics[assignment["name"]] = rubric_content
        except Exception as e:
            duck_logger.warn(f"Error loading the rubric files: {e}")
            raise e

    def _get_project_name_directly_from_report(self, report_contents, valid_project_names):
        report_contents = markdowndata.loads(report_contents)
        top_headers = report_contents.keys()
        for header in top_headers:
            if header in valid_project_names:
                return header
        return None

    async def _get_project_name_using_agent(self, context, report_contents, valid_project_names):
        input = {
            'report_contents': report_contents,
            'valid_projects_names': valid_project_names
        }

        response = await self._ai_client.run_agent(context, self._project_scanner_agent, str(input))
        response = json.loads(response)  # returns structured output as specified in the config
        project = response["project_name"]

        if project not in valid_project_names:
            raise ConversationComplete(
                f"The project report uploaded is not supported for grading.")

        return project

    async def _get_project_name_from_report(self, context, report_contents: str, valid_project_names: list[str]) -> str:
        if project_name := self._get_project_name_directly_from_report(report_contents, valid_project_names):
            return project_name
        else:
            return await self._get_project_name_using_agent(context, report_contents, valid_project_names)

    async def _get_report_contents(self, context):
        message = "Please upload your md report."
        for _ in range(3):
            await self._send_message(context.thread_id, message)
            response = await wait_for_message(context.timeout)
            if response is None:
                raise ConversationComplete("This conversation has timed out.")
            attachments = response.get("files", [])
            md_attachments = [attachment for attachment in attachments if "md" in attachment["filename"]]

            if not md_attachments:
                message = "No md files were uploaded. Please upload your md report: "
                continue

            file_contents = "\n".join([await self.read_url(attachment['url']) for attachment in md_attachments])
            return file_contents
        raise ConversationComplete("No md report was not uploaded")

    def _report_section_not_filled_in(self, report_section):
        cleaned_report_section = re.sub(r"[^A-Za-z0-9\s]", "", str(report_section))
        return cleaned_report_section.strip().lower() == "fill me in"

    async def _single_item_grader(self, context, piece_name, report_section, rubric_item) -> tuple[
        list[SECTION_NAME], RubricItemResponse]:

        if self._report_section_not_filled_in(report_section):
            return piece_name, RubricItemResponse(
                rubric_item=rubric_item,
                justification="Report section is not filled in.",
                satisfactory=False
            )

        input = {"report_contents": report_section,
                 "rubric_item": rubric_item}
        raw_response = await self._ai_client.run_agent(context, self._single_rubric_item_grader, str(input))
        result = json.loads(raw_response)  # expected response = RubricItemResponse
        return piece_name, result

    def _dict_to_md(self, d, level=1):
        """
        Convert a nested dict of the form
        {a: {b: {c: [1]}}}
        into markdown:

        # a
        ## b
        ### c
        - 1
        """
        lines = []

        for key, value in d.items():
            # Header line
            header_prefix = "#" * level
            lines.append(f"{header_prefix} {key}")

            # Nested dict → go one level deeper
            if isinstance(value, dict):
                lines.append(self._dict_to_md(value, level + 1))

            # List → bullet items or recurse
            elif isinstance(value, list):
                for item in value:
                    lines.append(f"- {item}")

            # Fallback: non-dict, non-list leaf
            else:
                lines.append(f"- {value}")

        return "\n".join(lines)
